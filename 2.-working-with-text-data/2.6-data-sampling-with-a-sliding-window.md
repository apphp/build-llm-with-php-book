# 2.6 Data sampling with a sliding window

When we prepare training data for a Large Language Model (LLM), we cannot simply give the model the whole text at once. Instead, we must create many small examples that the model can learn from. Each example contains two parts:

1. **Input** – what the model receives
2. **Target** – what the model must predict

LLMs learn by predicting the _next token_ (a token can be a word or a sub-word) based on the previous ones. During training, the model repeatedly sees pairs of sequences:

* The **input sequence** is a block of text.
* The **target sequence** is the same block, but shifted one token to the right.

### **How the sliding window works**

Imagine that our text is very long — thousands of tokens. We need to break it into many smaller pieces of length _max\_length_, because models can only process limited context.

To do this, we use a **sliding window**:

1. Take a block of tokens from position _i_ to _i + max\_length_.
2. Then move the window forward by _stride_ tokens.
3. Take the next block.
4. Repeat until we reach the end of the text.

This method produces many overlapping samples. It helps the model learn because each example contains slightly different contexts.

For every window:

* The **input** contains tokens from positions `[i ... i + max_length - 1]`
* The **target** contains tokens from positions `[i+1 ... i + max_length]`

So the model learns to predict each next token inside the window.

<div align="left"><figure><img src="../.gitbook/assets/2.6-data-sampling.png" alt="" width="563"><figcaption><p>In this case, the input to the LLM consists of blocks of text that lead to input to the models, and the task of the LLM during training is to predict the whole word that follows the input block.</p></figcaption></figure></div>





\>>>>>>>>>



